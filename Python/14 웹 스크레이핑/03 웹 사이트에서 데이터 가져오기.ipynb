{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순위 데이터를 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 사이트 순위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Tmall.com']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking[1:]]\n",
    "website_ranking_address[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Tmall.com',\n",
       " 'Facebook.com',\n",
       " 'Google.co.kr',\n",
       " 'Kakao.com',\n",
       " 'Amazon.com',\n",
       " 'Namu.wiki',\n",
       " 'Wikipedia.org',\n",
       " 'Sohu.com',\n",
       " 'Qq.com',\n",
       " 'Netflix.com',\n",
       " 'Taobao.com',\n",
       " '360.cn',\n",
       " 'Jd.com',\n",
       " 'Coupang.com',\n",
       " 'Dcinside.com',\n",
       " 'Apple.com',\n",
       " 'Baidu.com',\n",
       " 'Microsoft.com',\n",
       " 'Twitch.tv',\n",
       " 'Gmarket.co.kr',\n",
       " 'Donga.com',\n",
       " 'Yahoo.com',\n",
       " 'Adobe.com',\n",
       " 'Instagram.com',\n",
       " 'Sina.com.cn',\n",
       " 'Weibo.com',\n",
       " 'Nate.com',\n",
       " 'Zoom.us',\n",
       " '11st.co.kr',\n",
       " 'Stackoverflow.com',\n",
       " 'Office.com',\n",
       " 'Amazon.co.uk',\n",
       " 'Ebay.com',\n",
       " 'Bing.com',\n",
       " 'Aliexpress.com',\n",
       " 'Fmkorea.com',\n",
       " 'Afreecatv.com',\n",
       " 'Yna.co.kr',\n",
       " 'Chosun.com',\n",
       " 'Amazon.co.jp',\n",
       " 'Ppomppu.co.kr',\n",
       " 'Inven.co.kr',\n",
       " 'Ruliweb.com',\n",
       " 'Dropbox.com',\n",
       " 'Tumblr.com']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address = []\n",
    "\n",
    "for website_ranking_element in website_ranking[1:]:\n",
    "    website_ranking_address.append(website_ranking_element.get_text())\n",
    "    \n",
    "website_ranking_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1 Google.com\n",
      "2 Naver.com\n",
      "3 Youtube.com\n",
      "4 Daum.net\n",
      "5 Tistory.com\n",
      "6 Tmall.com\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "# p 태그의 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking[1:]]\n",
    "\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "for k in range(6):\n",
    "    # print(\"{0}: {1}\".format(k+1, website_ranking_address[k]))\n",
    "    print(f\"{k+1} {website_ranking_address[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tmall.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Website\n",
       "1   Google.com\n",
       "2    Naver.com\n",
       "3  Youtube.com\n",
       "4     Daum.net\n",
       "5  Tistory.com\n",
       "6    Tmall.com"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "website_ranking_dict = {\"Website\": website_ranking_address}\n",
    "df = pd.DataFrame(website_ranking_dict, columns=[\"Website\"], index=range(1, len(website_ranking_address)+1))\n",
    "df[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주간 음악 순위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">눈 (Feat. 이문세)</span>,\n",
       " <span class=\"ellipsis\">기억의 빈자리</span>,\n",
       " <span class=\"ellipsis\">선물</span>,\n",
       " <span class=\"ellipsis\">Beautiful</span>,\n",
       " <span class=\"ellipsis\">좋아</span>,\n",
       " <span class=\"ellipsis\">피카부 (Peek-A-Boo)</span>,\n",
       " <span class=\"ellipsis\">좋니</span>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색하고 싶은 년도를 입력하세요 :2020\n",
      "검색하고 싶은 월을 입력하세요 :1\n",
      "검색하고 싶은 주를 입력하세요 :1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">METEOR</span>,\n",
       " <span class=\"ellipsis\">Psycho</span>,\n",
       " <span class=\"ellipsis\">Blueming</span>,\n",
       " <span class=\"ellipsis\">HIP</span>,\n",
       " <span class=\"ellipsis\">Square (2017)</span>,\n",
       " <span class=\"ellipsis\">아마두 (feat.우원재, 김효은, 넉살, Huckleberry P)</span>,\n",
       " <span class=\"ellipsis\">Love poem</span>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input(\"검색하고 싶은 년도를 입력하세요 :\")\n",
    "month = input(\"검색하고 싶은 월을 입력하세요 :\")\n",
    "week = input(\"검색하고 싶은 주를 입력하세요 :\")\n",
    "\n",
    "if len(month) == 1:\n",
    "    month = \"0\" + month\n",
    "else:\n",
    "    month = month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색하고 싶은 년도를 입력하세요 :2020\n",
      "검색하고 싶은 월을 입력하세요 :1\n",
      "검색하고 싶은 주를 입력하세요 :1\n",
      "METEOR\n",
      "Psycho\n",
      "Blueming\n",
      "HIP\n",
      "Square (2017)\n",
      "아마두 (feat.우원재, 김효은, 넉살, Huckleberry P)\n",
      "Love poem\n",
      "어떻게 이별까지 사랑하겠어, 널 사랑하는 거지\n",
      "늦은 밤 너의 집 앞 골목길에서\n",
      "흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input(\"검색하고 싶은 년도를 입력하세요 :\")\n",
    "month = input(\"검색하고 싶은 월을 입력하세요 :\")\n",
    "week = input(\"검색하고 싶은 주를 입력하세요 :\")\n",
    "\n",
    "if len(month) == 1:\n",
    "    month = \"0\" + month\n",
    "else:\n",
    "    month = month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "\n",
    "for title in titles[0:10]:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색하고 싶은 년도를 입력하세요 :2020\n",
      "검색하고 싶은 월을 입력하세요 :1\n",
      "검색하고 싶은 주를 입력하세요 :1\n",
      "[ 2020년 01월 1주 Top 10 in Naver Music ]\n",
      "1위 METEOR\n",
      "2위 Psycho\n",
      "3위 Blueming\n",
      "4위 HIP\n",
      "5위 Square (2017)\n",
      "6위 아마두 (feat.우원재, 김효은, 넉살, Huckleberry P)\n",
      "7위 Love poem\n",
      "8위 어떻게 이별까지 사랑하겠어, 널 사랑하는 거지\n",
      "9위 늦은 밤 너의 집 앞 골목길에서\n",
      "10위 흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input(\"검색하고 싶은 년도를 입력하세요 :\")\n",
    "month = input(\"검색하고 싶은 월을 입력하세요 :\")\n",
    "week = input(\"검색하고 싶은 주를 입력하세요 :\")\n",
    "\n",
    "if len(month) == 1:\n",
    "    month = \"0\" + month\n",
    "else:\n",
    "    month = month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles_address = [title_element.get_text() for title_element in titles[0:10]]\n",
    "\n",
    "print(f\"[ {year}년 {month}월 {week}주 Top 10 in Naver Music ]\")\n",
    "for k in range(10):\n",
    "    print(f\"{k+1}위 {titles_address[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "년,월,주을 예시와 같이 입력해주세요. 예시) 2020 11 12020 11 1\n",
      "[ 2020년 11월 1주 Top 10 in Naver Music ]\n",
      "1위 Dynamite\n",
      "2위 DON'T TOUCH ME\n",
      "3위 Lovesick Girls\n",
      "4위 취기를 빌려 (취향저격 그녀 X 산들)\n",
      "5위 Savage Love (Laxed - Siren Beat) (BTS Remix)\n",
      "6위 마리아 (Maria)\n",
      "7위 When We Disco (Duet with 선미)\n",
      "8위 힘든 건 사랑이 아니다\n",
      "9위 에잇(Prod.&Feat. SUGA of BTS)\n",
      "10위 Make A Wish (Birthday Song)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year, month, week = map(str, input('년,월,주을 예시와 같이 입력해주세요. 예시) 2020 11 1').split())\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles_address = [title_element.get_text() for title_element in titles[0:10]]\n",
    "\n",
    "print(f\"[ {year}년 {month}월 {week}주 Top 10 in Naver Music ]\")\n",
    "for k in range(10):\n",
    "    print(f\"{k+1}위 {titles_address[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">눈 (Feat. 이문세)</span>,\n",
       " <span class=\"ellipsis\">기억의 빈자리</span>,\n",
       " <span class=\"ellipsis\">선물</span>,\n",
       " <span class=\"ellipsis\">Beautiful</span>,\n",
       " <span class=\"ellipsis\">좋아</span>,\n",
       " <span class=\"ellipsis\">피카부 (Peek-A-Boo)</span>,\n",
       " <span class=\"ellipsis\">좋니</span>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_titles = [title.get_text() for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', '기억의 빈자리', '선물', 'Beautiful', '좋아', '피카부 (Peek-A-Boo)', '좋니']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\tZion.T\\r\\n\\t\\t'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists = soup_music.select('a._artist span.ellipsis') \n",
    "artists[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zion.T'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[0].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신',\n",
       " '뉴이스트 W']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "music_artists[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zion.T'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists = soup_music.select('td._artist a')\n",
    "artists[0].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'민서'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[4].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " '민서',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "music_artists[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 눈 (Feat. 이문세) / Zion.T\n",
      "2: 기억의 빈자리 / 나얼\n",
      "3: 선물 / 멜로망스(Melomance)\n",
      "4: Beautiful / Wanna One(워너원)\n",
      "5: 좋아 / 민서\n",
      "6: 피카부 (Peek-A-Boo) / Red Velvet (레드벨벳)\n",
      "7: 좋니 / 윤종신\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=1\"    \n",
    "# url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=2\"\n",
    "# url = \"http://music.naver.com/listen/top100.nhn?domain=TOTAL&page=1\"\n",
    "    \n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "artists = soup_music.select('td._artist a')\n",
    "\n",
    "music_titles = [title.get_text() for title in titles]\n",
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "\n",
    "for k in range(7):\n",
    "    print(\"{0}: {1} / {2}\".format(k+1, music_titles[k], music_artists[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1등부터 50등까지, 51등부터 100등까지의 음악 순위 (선생님 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 눈 (Feat. 이문세) / Zion.T\n",
      "2: 기억의 빈자리 / 나얼\n",
      "3: 선물 / 멜로망스(Melomance)\n",
      "4: Beautiful / Wanna One(워너원)\n",
      "5: 좋아 / 민서\n",
      "6: 피카부 (Peek-A-Boo) / Red Velvet (레드벨벳)\n",
      "7: 좋니 / 윤종신\n",
      "-----------------------------------------\n",
      "51: 가시나 / 선미\n",
      "52: all of my life / 박원\n",
      "53: To Be One (Outro.) / Wanna One(워너원)\n",
      "54: 비행운 / 문문(MoonMoon)\n",
      "55: 고민보다 Go / 방탄소년단\n",
      "56: 빨간 맛 (Red Flavor) / Red Velvet (레드벨벳)\n",
      "57: 덜덜덜 / EXID\n",
      "58: 밤이 되니까 / 펀치 (Punch)\n",
      "59: 피 땀 눈물 / 방탄소년단\n",
      "60: Blue / 볼빨간사춘기\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url1to50 = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=1\"    \n",
    "url51to100 = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=2\"\n",
    "# url = \"http://music.naver.com/listen/top100.nhn?domain=TOTAL&page=1\"\n",
    "# url = \"http://music.naver.com/listen/top100.nhn?domain=TOTAL&page=2\"\n",
    "    \n",
    "html_music1to50 = requests.get(url1to50).text\n",
    "soup_music1to50 = BeautifulSoup(html_music1to50, \"lxml\")\n",
    "\n",
    "titles1to50 = soup_music1to50.select('a._title span.ellipsis') \n",
    "artists1to50 = soup_music1to50.select('td._artist a')\n",
    "\n",
    "music_titles1to50 = [title.get_text() for title in titles1to50]\n",
    "music_artists1to50 = [artist.get_text().strip() for artist in artists1to50 ]\n",
    "\n",
    "for k in range(7):\n",
    "    print(\"{0}: {1} / {2}\".format(k+1, music_titles1to50[k], music_artists1to50[k]))\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "html_music51to100 = requests.get(url51to100).text\n",
    "soup_music51to100 = BeautifulSoup(html_music51to100, \"lxml\")\n",
    "\n",
    "titles51to100 = soup_music51to100.select('a._title span.ellipsis') \n",
    "artists51to100 = soup_music51to100.select('td._artist a')\n",
    "\n",
    "music_titles51to100 = [title.get_text() for title in titles51to100]\n",
    "music_artists51to100 = [artist.get_text().strip() for artist in artists51to100]\n",
    "   \n",
    "num = 50\n",
    "for k in range(10):\n",
    "    print(str(num+1)+\": {0} / {1}\".format(music_titles51to100[k], music_artists51to100[k]))\n",
    "    num=num+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "music_titles = [title.get_text() for title in titles]\n",
    "\n",
    "artists = soup_music.select('td._artist a')\n",
    "music_artists = [artist.get_text().strip() for artist in artists]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_titles_artists={}\n",
    "order = 0\n",
    "\n",
    "for (music_title, music_artist) in zip(music_titles, music_artists):\n",
    "    order = order + 1\n",
    "    music_titles_artists[order] = [music_title, music_artist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', 'Zion.T']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기억의 빈자리', '나얼']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaverMusicTop100 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/myPyCode/data/NaverMusicTop100.txt']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "    \n",
    "naver_music_url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=\"\n",
    " \n",
    "# 네이버 music 주소를 입력하면 노래 제목과 아티스트를 반환\n",
    "def naver_music(url):    \n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "    titles = soup_music.select('a._title span.ellipsis') \n",
    "    artists = soup_music.select('td._artist a')\n",
    "\n",
    "    music_titles = [title.get_text() for title in titles]\n",
    "    music_artists = [artist.get_text().strip() for artist in artists]\n",
    "    \n",
    "    return music_titles, music_artists\n",
    "\n",
    "# 노래 제목과 아티스트를 저장할 파일 이름을 폴더와 함께 지정\n",
    "file_name = 'C:/myPyCode/data/NaverMusicTop100.txt'\n",
    "\n",
    "f = open(file_name,'w') # 파일 열기\n",
    "\n",
    "# 각 page에는 50개의 노래 제목과 아티스트가 추출됨\n",
    "for page in range(2):\n",
    "    naver_music_url_page = naver_music_url + str(page+1) # page URL\n",
    "    naver_music_titles, naver_music_artists = naver_music(naver_music_url_page)\n",
    "    \n",
    "    # 추출된 노래 제목과 아티스트를 파일에 저장 \n",
    "    for k in range(len(naver_music_titles)):\n",
    "        f.write(\"{0:2d}: {1}/{2}\\n\".format(page*50 + k+1, naver_music_titles[k],  naver_music_artists[k]))\n",
    "        \n",
    "f.close() # 파일 닫기\n",
    "\n",
    "glob.glob(file_name) # 생성된 파일 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 페이지에서 이미지 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하나의 이미지 내려받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  \n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "html_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/myPyCode/download' \n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path\n",
    "\n",
    "imageFile = open(image_path, 'wb')\n",
    "\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python-logo.png']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iyovvxr10b0', 'python-logo.png']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  \n",
    "\n",
    "ipad_url = \"https://search4.kakaocdn.net/argon/0x200_85_hr/Iyovvxr10b0\"\n",
    "html_image = requests.get(ipad_url)\n",
    "html_image\n",
    "\n",
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(ipad_url)\n",
    "\n",
    "folder = 'C:/myPyCode/download' \n",
    "\n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path\n",
    "\n",
    "imageFile = open(image_path, 'wb')\n",
    "\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()\n",
    "\n",
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20201104110532822ekul.jpg', 'Iyovvxr10b0', 'python-logo.png']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  \n",
    "\n",
    "url = 'https://img4.daumcdn.net/thumb/R658x0.q70/?fname=https://t1.daumcdn.net/news/202011/04/akn/20201104110532822ekul.jpg'\n",
    "html_image = requests.get(url)\n",
    "# print(html_image.text)\n",
    "html_image\n",
    "\n",
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name\n",
    "\n",
    "folder = 'C:/myPyCode/download' \n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path\n",
    "\n",
    "imageFile = open(image_path, 'wb')\n",
    "\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()\n",
    "\n",
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 이미지 내려받기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 픽사베이는 현재 크롤링이 안되는듯..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "URL = \"https://pixabay.com/photos/search/animal/?cat=animals\"\n",
    "\n",
    "html_pixabay_image = requests.get(URL).text\n",
    "soup_pixabay_image = BeautifulSoup(html_pixabay_image, \"lxml\")\n",
    "pixabay_image_elements = soup_pixabay_image.select('img') \n",
    "pixabay_image_elements[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reshot에서 이미지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Reshot navbar logo\" class=\"black\" src=\"https://assets-static.reshot-cdn.com/brand/reshot-logo.png\"/>,\n",
       " <img alt=\"Free authentic animal photo on Reshot\" src=\"https://res.cloudinary.com/twenty20/private_images/t_standard-fit/v1521838685/photosp/bae96789-a5ab-4471-b54f-9686ace09e33/bae96789-a5ab-4471-b54f-9686ace09e33.jpg\"/>,\n",
       " <img alt=\"Back off!\" src=\"https://res.cloudinary.com/twenty20/private_images/t_standard-fit/v1597098233/photosp/a44357c5-b1c3-41ef-9a65-7a4937b06a44/a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg\"/>,\n",
       " <img alt='\"Orphans\"' src=\"https://res.cloudinary.com/twenty20/private_images/t_standard-fit/v1597098206/photosp/34fd9c70-8996-4706-a0f1-113231ed3eee/34fd9c70-8996-4706-a0f1-113231ed3eee.jpg\"/>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "URL = 'https://www.reshot.com/search/animal'\n",
    "\n",
    "html_reshot_image = requests.get(URL).text\n",
    "# print(html_reshot_image)\n",
    "\n",
    "soup_reshot_image = BeautifulSoup(html_reshot_image, \"lxml\")\n",
    "# print(soup_reshot_image)\n",
    "\n",
    "reshot_image_elements = soup_reshot_image.select('a img') \n",
    "reshot_image_elements[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://res.cloudinary.com/twenty20/private_images/t_standard-fit/v1597098233/photosp/a44357c5-b1c3-41ef-9a65-7a4937b06a44/a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshot_image_url = reshot_image_elements[2].get('src')\n",
    "reshot_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(reshot_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = requests.get(reshot_image_url)\n",
    "\n",
    "folder = \"C:/myPyCode/download\"\n",
    "    \n",
    "# os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리하는 방법    \n",
    "imageFile = open(os.path.join(folder, os.path.basename(reshot_image_url)), 'wb')\n",
    "\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 저장하는 방법\n",
    "chunk_size = 1000000 \n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 이미지를 다운 받는 함수로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일명: 'reshot-logo.png'. 내려받기 완료!\n",
      "이미지 파일명: 'bae96789-a5ab-4471-b54f-9686ace09e33.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg'. 내려받기 완료!\n",
      "이미지 파일명: '34fd9c70-8996-4706-a0f1-113231ed3eee.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'dbd9fa3b-238b-47b1-8e20-c05400cbe921.jpg'. 내려받기 완료!\n",
      "이미지 파일명: '737d192f-ba38-4a71-9bb9-9d40b45d0263.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'c3c3604d-36eb-4f8a-9768-cebc0749d5a5.jpg'. 내려받기 완료!\n",
      "================================\n",
      "선택한 모든 이미지 내려받기 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "import os\n",
    "\n",
    "# URL(주소)에서 이미지 주소 추출\n",
    "def get_image_url(url): \n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")  \n",
    "    image_elements = soup_image_url.select('a img') \n",
    "    if(image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls        \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):  \n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리    \n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')\n",
    "\n",
    "        chunk_size = 1000000 # 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "            imageFile.close()\n",
    "        print(\"이미지 파일명: '{0}'. 내려받기 완료!\".format(os.path.basename(img_url))) \n",
    "    else:       \n",
    "        print(\"내려받을 이미지가 없습니다.\")\n",
    "        \n",
    "# 웹 사이트의 주소 지정   \n",
    "reshot_url = 'https://www.reshot.com/search/animal'\n",
    "# pixabay_url= 'https://pixabay.com/ko/photos/?order=popular&cat=animals&pagi=2'\n",
    "\n",
    "figure_folder = \"C:/myPyCode/download\" # 이미지를 내려받을 폴더 지정  \n",
    "\n",
    "reshot_image_urls = get_image_url(reshot_url) # 이미지 파일의 주소 가져오기\n",
    "\n",
    "num_of_download_image = 7 # 내려받을 이미지 개수 지정\n",
    "# num_of_download_image = len(pixabay_image_urls)\n",
    "\n",
    "for k in range(num_of_download_image):\n",
    "    download_image(figure_folder,reshot_image_urls[k])\n",
    "print(\"================================\")\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 폴더 만드는 함수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일명: 'reshot-logo.png'. 내려받기 완료!\n",
      "이미지 파일명: 'bae96789-a5ab-4471-b54f-9686ace09e33.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg'. 내려받기 완료!\n",
      "이미지 파일명: '34fd9c70-8996-4706-a0f1-113231ed3eee.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'dbd9fa3b-238b-47b1-8e20-c05400cbe921.jpg'. 내려받기 완료!\n",
      "이미지 파일명: '737d192f-ba38-4a71-9bb9-9d40b45d0263.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'c3c3604d-36eb-4f8a-9768-cebc0749d5a5.jpg'. 내려받기 완료!\n",
      "================================\n",
      "선택한 모든 이미지 내려받기 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "import os\n",
    "\n",
    "# URL(주소)에서 이미지 주소 추출\n",
    "def get_image_url(url): \n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")  \n",
    "    image_elements = soup_image_url.select('a img') \n",
    "    if(image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls        \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):  \n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리    \n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')\n",
    "\n",
    "        chunk_size = 1000000 # 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "            imageFile.close()\n",
    "        print(\"이미지 파일명: '{0}'. 내려받기 완료!\".format(os.path.basename(img_url))) \n",
    "    else:       \n",
    "        print(\"내려받을 이미지가 없습니다.\")\n",
    "        \n",
    "# 웹 사이트의 주소 지정   \n",
    "reshot_url = 'https://www.reshot.com/search/animal'\n",
    "\n",
    "figure_folder = 'C:/myPyCode/download2' \n",
    "\n",
    "if not os.path.exists(figure_folder):\n",
    "    os.makedirs(figure_folder)\n",
    "\n",
    "reshot_image_urls = get_image_url(reshot_url) # 이미지 파일의 주소 가져오기\n",
    "\n",
    "num_of_download_image = 7 # 내려받을 이미지 개수 지정\n",
    "\n",
    "for k in range(num_of_download_image):\n",
    "    download_image(figure_folder,reshot_image_urls[k])\n",
    "print(\"================================\")\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
