{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순위 데이터를 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 사이트 순위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Tmall.com']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking[1:]]\n",
    "website_ranking_address[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Tmall.com',\n",
       " 'Facebook.com',\n",
       " 'Google.co.kr',\n",
       " 'Kakao.com',\n",
       " 'Amazon.com',\n",
       " 'Namu.wiki',\n",
       " 'Wikipedia.org',\n",
       " 'Sohu.com',\n",
       " 'Qq.com',\n",
       " 'Netflix.com',\n",
       " 'Taobao.com',\n",
       " '360.cn',\n",
       " 'Jd.com',\n",
       " 'Coupang.com',\n",
       " 'Dcinside.com',\n",
       " 'Apple.com',\n",
       " 'Baidu.com',\n",
       " 'Microsoft.com',\n",
       " 'Twitch.tv',\n",
       " 'Gmarket.co.kr',\n",
       " 'Donga.com',\n",
       " 'Yahoo.com',\n",
       " 'Adobe.com',\n",
       " 'Instagram.com',\n",
       " 'Sina.com.cn',\n",
       " 'Weibo.com',\n",
       " 'Nate.com',\n",
       " 'Zoom.us',\n",
       " '11st.co.kr',\n",
       " 'Stackoverflow.com',\n",
       " 'Office.com',\n",
       " 'Amazon.co.uk',\n",
       " 'Ebay.com',\n",
       " 'Bing.com',\n",
       " 'Aliexpress.com',\n",
       " 'Fmkorea.com',\n",
       " 'Afreecatv.com',\n",
       " 'Yna.co.kr',\n",
       " 'Chosun.com',\n",
       " 'Amazon.co.jp',\n",
       " 'Ppomppu.co.kr',\n",
       " 'Inven.co.kr',\n",
       " 'Ruliweb.com',\n",
       " 'Dropbox.com',\n",
       " 'Tumblr.com']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address = []\n",
    "\n",
    "for website_ranking_element in website_ranking[1:]:\n",
    "    website_ranking_address.append(website_ranking_element.get_text())\n",
    "    \n",
    "website_ranking_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1 Google.com\n",
      "2 Naver.com\n",
      "3 Youtube.com\n",
      "4 Daum.net\n",
      "5 Tistory.com\n",
      "6 Tmall.com\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "# p 태그의 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking[1:]]\n",
    "\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "for k in range(6):\n",
    "    # print(\"{0}: {1}\".format(k+1, website_ranking_address[k]))\n",
    "    print(f\"{k+1} {website_ranking_address[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tmall.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Website\n",
       "1   Google.com\n",
       "2    Naver.com\n",
       "3  Youtube.com\n",
       "4     Daum.net\n",
       "5  Tistory.com\n",
       "6    Tmall.com"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "website_ranking_dict = {\"Website\": website_ranking_address}\n",
    "df = pd.DataFrame(website_ranking_dict, columns=[\"Website\"], index=range(1, len(website_ranking_address)+1))\n",
    "df[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주간 음악 순위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">눈 (Feat. 이문세)</span>,\n",
       " <span class=\"ellipsis\">기억의 빈자리</span>,\n",
       " <span class=\"ellipsis\">선물</span>,\n",
       " <span class=\"ellipsis\">Beautiful</span>,\n",
       " <span class=\"ellipsis\">좋아</span>,\n",
       " <span class=\"ellipsis\">피카부 (Peek-A-Boo)</span>,\n",
       " <span class=\"ellipsis\">좋니</span>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색하고 싶은 년도를 입력하세요 :2020\n",
      "검색하고 싶은 월을 입력하세요 :1\n",
      "검색하고 싶은 주를 입력하세요 :1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">METEOR</span>,\n",
       " <span class=\"ellipsis\">Psycho</span>,\n",
       " <span class=\"ellipsis\">Blueming</span>,\n",
       " <span class=\"ellipsis\">HIP</span>,\n",
       " <span class=\"ellipsis\">Square (2017)</span>,\n",
       " <span class=\"ellipsis\">아마두 (feat.우원재, 김효은, 넉살, Huckleberry P)</span>,\n",
       " <span class=\"ellipsis\">Love poem</span>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input(\"검색하고 싶은 년도를 입력하세요 :\")\n",
    "month = input(\"검색하고 싶은 월을 입력하세요 :\")\n",
    "week = input(\"검색하고 싶은 주를 입력하세요 :\")\n",
    "\n",
    "if len(month) == 1:\n",
    "    month = \"0\" + month\n",
    "else:\n",
    "    month = month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색하고 싶은 년도를 입력하세요 :2020\n",
      "검색하고 싶은 월을 입력하세요 :1\n",
      "검색하고 싶은 주를 입력하세요 :1\n",
      "METEOR\n",
      "Psycho\n",
      "Blueming\n",
      "HIP\n",
      "Square (2017)\n",
      "아마두 (feat.우원재, 김효은, 넉살, Huckleberry P)\n",
      "Love poem\n",
      "어떻게 이별까지 사랑하겠어, 널 사랑하는 거지\n",
      "늦은 밤 너의 집 앞 골목길에서\n",
      "흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input(\"검색하고 싶은 년도를 입력하세요 :\")\n",
    "month = input(\"검색하고 싶은 월을 입력하세요 :\")\n",
    "week = input(\"검색하고 싶은 주를 입력하세요 :\")\n",
    "\n",
    "if len(month) == 1:\n",
    "    month = \"0\" + month\n",
    "else:\n",
    "    month = month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "\n",
    "for title in titles[0:10]:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색하고 싶은 년도를 입력하세요 :2020\n",
      "검색하고 싶은 월을 입력하세요 :1\n",
      "검색하고 싶은 주를 입력하세요 :1\n",
      "[ 2020년 01월 1주 Top 10 in Naver Music ]\n",
      "1위 METEOR\n",
      "2위 Psycho\n",
      "3위 Blueming\n",
      "4위 HIP\n",
      "5위 Square (2017)\n",
      "6위 아마두 (feat.우원재, 김효은, 넉살, Huckleberry P)\n",
      "7위 Love poem\n",
      "8위 어떻게 이별까지 사랑하겠어, 널 사랑하는 거지\n",
      "9위 늦은 밤 너의 집 앞 골목길에서\n",
      "10위 흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input(\"검색하고 싶은 년도를 입력하세요 :\")\n",
    "month = input(\"검색하고 싶은 월을 입력하세요 :\")\n",
    "week = input(\"검색하고 싶은 주를 입력하세요 :\")\n",
    "\n",
    "if len(month) == 1:\n",
    "    month = \"0\" + month\n",
    "else:\n",
    "    month = month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles_address = [title_element.get_text() for title_element in titles[0:10]]\n",
    "\n",
    "print(f\"[ {year}년 {month}월 {week}주 Top 10 in Naver Music ]\")\n",
    "for k in range(10):\n",
    "    print(f\"{k+1}위 {titles_address[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
